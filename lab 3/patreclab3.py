# -*- coding: utf-8 -*-
"""patreclab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zTAXmxLzh5v7MhfaWd4QlseHlBuiMArm

# Migrate from Kaggle
"""

# run every time

!pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""# Βήμα 0"""

! kaggle datasets download -d geoparslp/patreco3-multitask-affective-music

! mkdir kaggle
! mkdir kaggle/input
! mkdir kaggle/input/patreco3-multitask-affective-music
! unzip patreco3-multitask-affective-music.zip -d kaggle/input/patreco3-multitask-affective-music

! pwd
! ls

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('./kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Βήμα 1"""

data_lab = open('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train_labels.txt', 'r')
print(data_lab.read())

#we chose one Blues file and one Electronic
#Blues : 123947.fused.full.npy.gz
#Electronic : 14780.fused.full.npy.gz

Blues = np.load('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/123947.fused.full.npy')
Electronic = np.load('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/14780.fused.full.npy')

print(Blues.shape) 
print(Electronic.shape)

Blues_mel, Blues_chroma = Blues[:128], Blues[128:]
Electronic_mel, Electronic_chroma = Electronic[:128], Electronic[128:]

print(Blues_mel.shape, Blues_chroma.shape)
print(Electronic_mel.shape, Electronic_chroma.shape)

# Plot the spectrogram and the chromagram
#BLUES

import librosa.display
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
img = librosa.display.specshow(Blues_mel, x_axis='time', y_axis='linear', ax=ax)
ax.set(title='Spectrogram')
# fig.colorbar(img, ax=ax, format="%+2.f dB")

# Plot the spectrogram and the chromagram
#ELECTRONIC

import librosa.display
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
img = librosa.display.specshow(Electronic_mel, x_axis='time', y_axis='linear', ax=ax)
ax.set(title='Spectrogram')
# fig.colorbar(img, ax=ax, format="%+2.f dB")

"""Τα spectrograms αποτελούν μια οπτική αναπαράσταση του spectrum των συχνοτήτων ενός σήματος, καθώς αυτό μεταβάλλεται με τον χρόνο. Το χρώμα που παρατηρείται ακολουθεί τις τιμές του πλαινού πίνακα και αναπαριστά το amplitude μιας συγκεκριμένης συχνότητας σε μία συγκεκριμένη χρονική στιγμή. Στις συγκεκριμένες αναπαραστάσεις, παρατηρούμε ότι το πάνω σπεκτρόγραμμα που αναπαριστά ένα κομμάτι blues, έχει πολλές μεταβολές από τις υψηλές συχνότητες στις χαμηλές, αλλά με μια τάση στις χαμηλές συχνότητες. Αντίθετα, στην περίπτωση της ηλεκτρονικής μουσικής, έχουμε χρήση και υψηλότερων συχνοτήτων, καθώς επίσης παρατηρείται και μία συνέχεια.

# Βήμα 2
"""

print(Blues_mel.shape)
print(Electronic_mel.shape)

"""Παρέχεται αναλυτικός σχολιασμός στην αναφορά μας."""

Blues_beat = np.load('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/123947.fused.full.npy')
Electronic_beat = np.load('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/14780.fused.full.npy')

Blues_beat_mel, Blues_beat_chroma = Blues_beat[:128], Blues_beat[128:]
Electronic_beat_mel, Electronic_beat_chroma = Electronic_beat[:128], Electronic_beat[128:]

print(Blues_beat_mel.shape, Blues_beat_chroma.shape)
print(Electronic_beat_mel.shape, Electronic_beat_chroma.shape)

"""Παρατηρούμε ότι οι διαστάσεις πράγματι μειώθηκαν."""

# Plot the spectrogram and the chromagram
#BLUES

import librosa.display
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
img = librosa.display.specshow(Blues_beat_mel, x_axis='time', y_axis='linear', ax=ax)
ax.set(title='Spectrogram')
# fig.colorbar(img, ax=ax, format="%+2.f dB")

# Plot the spectrogram and the chromagram
#ELECTRONIC

import librosa.display
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
img = librosa.display.specshow(Electronic_beat_mel, x_axis='time', y_axis='linear', ax=ax)
ax.set(title='Spectrogram')
# fig.colorbar(img, ax=ax, format="%+2.f dB")

"""Φαίνεται να διατηρείται ίδια η μορφή και άρα και η πληροφορία που έχουμε, παρά την μείωση των χρονικών βημάτων. Η μείωση αυτή γίνεται φανερή και πάνω στα spectrograms.

# Βήμα 3
"""

fig, ax = plt.subplots()
img = librosa.display.specshow(Blues_chroma, y_axis='chroma', x_axis='time', ax=ax)
ax.set(title='Chromagram')
# fig.colorbar(img, ax=ax)

fig, ax = plt.subplots()
img = librosa.display.specshow(Electronic_chroma, y_axis='chroma', x_axis='time', ax=ax)
ax.set(title='Chromagram')
# fig.colorbar(img, ax=ax)

fig, ax = plt.subplots()
img = librosa.display.specshow(Blues_beat_chroma, y_axis='chroma', x_axis='time', ax=ax)
ax.set(title='Chromagram')
# fig.colorbar(img, ax=ax)

fig, ax = plt.subplots()
img = librosa.display.specshow(Electronic_beat_chroma, y_axis='chroma', x_axis='time', ax=ax)
ax.set(title='Chromagram')
# fig.colorbar(img, ax=ax)

"""Παρατηρούμε και εδώ ότι τα beat synchronized αρχεία μας δίνουν παρόμοια εικόνα με τα πρωτότυπα, παρά την μείωση των χρονικών βημάτων. Βλέπουμε ότι απλώς γίνεται πιο "θολή" η εικόνα, διατηρώντας όμως την πληροφορία που μας παρέχεται από τους χρωματισμούς στις αντίστοιχες νότες. Περισσότερη ανάλυση σχετικά με τα χρωμογράμματα υπάρχει στην αναφορά μας.

# Βήμα 4
"""

# Import the necessary libraries

import numpy as np
import copy
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
from torch.utils.data import SubsetRandomSampler, DataLoader
import re

# Combine similar classes and remove underrepresented classes
class_mapping = {
    'Rock': 'Rock',
    'Psych-Rock': 'Rock',
    'Indie-Rock': None,
    'Post-Rock': 'Rock',
    'Psych-Folk': 'Folk',
    'Folk': 'Folk',
    'Metal': 'Metal',
    'Punk': 'Metal',
    'Post-Punk': None,
    'Trip-Hop': 'Trip-Hop',
    'Pop': 'Pop',
    'Electronic': 'Electronic',
    'Hip-Hop': 'Hip-Hop',
    'Classical': 'Classical',
    'Blues': 'Blues',
    'Chiptune': 'Electronic',
    'Jazz': 'Jazz',
    'Soundtrack': None,
    'International': None,
    'Old-Time': None
}


# Helper functions to read fused, mel, and chromagram
def read_fused_spectrogram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)
    return spectrogram.T


def read_mel_spectrogram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)[:128]
    return spectrogram.T

    
def read_chromagram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)[128:]
    return spectrogram.T

import os

# TODO: Comment on how the train and validation splits are created.
# TODO: It's useful to set the seed when debugging but when experimenting ALWAYS set seed=None. Why?
def torch_train_val_split(
        dataset, batch_train, batch_eval,
        val_size=.2, shuffle=True, seed=None):
    # Creating data indices for training and validation splits:
    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    val_split = int(np.floor(val_size * dataset_size))
    if shuffle:
        np.random.seed(seed)
        np.random.shuffle(indices)
    train_indices = indices[val_split:]
    val_indices = indices[:val_split]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    train_loader = DataLoader(dataset,
                              batch_size=batch_train,
                              sampler=train_sampler)
    val_loader = DataLoader(dataset,
                            batch_size=batch_eval,
                            sampler=val_sampler)
    return train_loader, val_loader


class LabelTransformer(LabelEncoder):
    def inverse(self, y):
        try:
            return super(LabelTransformer, self).inverse_transform(y)
        except:
            return super(LabelTransformer, self).inverse_transform([y])

    def transform(self, y):
        try:
            return super(LabelTransformer, self).transform(y)
        except:
            return super(LabelTransformer, self).transform([y])


# TODO: Comment on why padding is needed
class PaddingTransform(object):
    def __init__(self, max_length, padding_value=0):
        self.max_length = max_length
        self.padding_value = padding_value

    def __call__(self, s):
        if len(s) == self.max_length:
            return s

        if len(s) > self.max_length:
            return s[:self.max_length]

        if len(s) < self.max_length:
            s1 = copy.deepcopy(s)
            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)
            s1 = np.vstack((s1, pad))
            return s1

# Pytorch Dataset Class for creating the dataset
class SpectrogramDataset(Dataset):
    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_fused_spectrogram):
        t = 'train' if train else 'test'
        p = os.path.join(path, t)
        self.index = os.path.join(path, "{}_labels.txt".format(t))
        self.files, labels = self.get_files_labels(self.index, class_mapping)
        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]
        self.feat_dim = self.feats[0].shape[1]
        self.lengths = [len(i) for i in self.feats]
        self.max_length = max(self.lengths) if max_length <= 0 else max_length
        self.zero_pad_and_stack = PaddingTransform(self.max_length)
        self.label_transformer = LabelTransformer()
        if isinstance(labels, (list, tuple)):
            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')

    def get_files_labels(self, txt, class_mapping):
        with open(txt, 'r') as fd:
            lines = [l.rstrip().split('\t') for l in fd.readlines()[1:]]
        files, labels = [], []
        for l in lines:
            label = l[1]
            if class_mapping:
                label = class_mapping[l[1]]
            if not label:
                continue
            # Kaggle automatically unzips the npy.gz format so this hack is needed
            _id = l[0].split('.')[0]
            npy_file = '{}.fused.full.npy'.format(_id)
            files.append(npy_file)
            labels.append(label)
        return files, labels

    def __getitem__(self, item):
        # TODO: Inspect output and comment on how the output is formatted
        l = min(self.lengths[item], self.max_length)
        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l

    def __len__(self):
        return len(self.labels)

beat_mel_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)
    
train_loader_beat_mel, val_loader_beat_mel = torch_train_val_split(beat_mel_specs, 32 ,32, val_size=.33)
     
ttest_loader_beat_mel = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

beat_chroma = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

train_loader_beat_chroma, val_loader_beat_chroma = torch_train_val_split(beat_chroma, 32 ,32, val_size=.33)

ttest_loader_beat_chroma = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

specs_fused = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_fused_spectrogram)

train_loader, val_loader = torch_train_val_split(specs_fused, 32 ,32, val_size=.33)

ttest_loader = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
     train=False,
     class_mapping=class_mapping,
     max_length=-1,
     read_spec_fn=read_fused_spectrogram)

import matplotlib.pyplot as plt

#we create an histogram before the classes mixing

data_lab = open('./kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train_labels.txt', 'r')
data_lab.readline()
labels_before = []
for line in data_lab:
    labels_before.append(line.split()[1])

unique_labels = list(set(labels_before)) #we save the different kind of music we have
print(unique_labels)

occur = []
for i in range(len(unique_labels)):
    occur.append(labels_before.count(unique_labels[i])) #we find the occurences of every kind in the whole array of labels

print(occur)

fig = plt.figure(figsize = (20,15))
plt.bar(unique_labels,occur,color='blue')

labels_after = list(set(class_mapping.values()))
labels_after.remove(None)
print(labels_after)

occur_after = np.zeros(len(labels_after))

for kind in class_mapping.keys() :
    #print(class_mapping[kind])
    if(class_mapping[kind]!=None):
        temp = labels_before.count(kind)
        #print(temp)
        index = labels_after.index(class_mapping[kind])
        #print(index)
        occur_after[index] = occur_after[index] + temp

print(occur_after)

fig = plt.figure(figsize = (20,15))
plt.bar(labels_after,occur_after,color='blue')

"""# Βήμα 5

Αρχικά, φορτώνουμε τον κώδικα του LSTM που υλοποιήσαμε στο προηγούμενο εργαστήριο :
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import torch.nn as nn

class BasicLSTM(nn.Module):
    def __init__(self, input_dim, rnn_size, output_dim, num_layers = 1, bidirectional=False, dropout_probability=0):
        super(BasicLSTM, self).__init__()
        self.bidirectional = bidirectional
        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size

        # --------------- Insert your code here ---------------- #
        # Initialize the LSTM, Dropout, Output layers
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.dropout_probability = dropout_probability
        self.rnn_size = rnn_size
        self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.rnn_size, num_layers=self.num_layers,
                            batch_first=True, dropout=self.dropout_probability, bidirectional=self.bidirectional)
        self.linear = nn.Linear(self.feature_size, self.output_dim)
        #self.soft = nn.Softmax(dim=1)


    def forward(self, x, lengths):
        """ 
            x : 3D numpy array of dimension N x L x D
                N: batch index
                L: sequence index
                D: feature index
            lengths: N x 1
         """
        
        # --------------- Insert your code here ---------------- #
        
        # You must have all of the outputs of the LSTM, but you need only the last one (that does not exceed the sequence length)
        # To get it use the last_timestep method
        # Then pass it through the remaining network

        batch_size = len(x)
        seq_len = len(x[0])

        # initialize the hidden state of lstm
        num_of_directions = 1
        if self.bidirectional == True:
          num_of_directions = 2
        hidden = torch.randn(self.num_layers * num_of_directions, batch_size, self.rnn_size).to(device)
        cell_state = torch.randn(self.num_layers * num_of_directions, batch_size, self.rnn_size).to(device)
        #cell_state = init_cell_state(batch_size)

        # pass our data and state to lstm
        outputs, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))

        last_outputs = self.last_timestep(outputs, lengths, self.bidirectional)
        last_outputs = self.linear(last_outputs)
        #last_outputs = self.soft(last_outputs)
        return last_outputs


    def last_timestep(self, outputs, lengths, bidirectional=False):
        """
            Returns the last output of the LSTM taking into account the zero padding
        """
        if bidirectional:
            forward, backward = self.split_directions(outputs)
            last_forward = self.last_by_index(forward, lengths)
            last_backward = backward[:, 0, :]
            # Concatenate and return - maybe add more functionalities like average
            return torch.cat((last_forward, last_backward), dim=-1)

        else:
            return self.last_by_index(outputs, lengths)

    @staticmethod
    def split_directions(outputs):
        direction_size = int(outputs.size(-1) / 2)
        forward = outputs[:, :, :direction_size]
        backward = outputs[:, :, direction_size:]
        return forward, backward

    @staticmethod
    def last_by_index(outputs, lengths):
        # Index of the last output for each sequence.
        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),
                                               outputs.size(2)).unsqueeze(1).to(device)
        return outputs.gather(1, idx).squeeze().to(device)

def label_to_one_hot(label, number_of_labels):
  code = []
  for i in range(number_of_labels):
    if i == label:
      code.append(1)
    else:
      code.append(0)
  return code

import torch
from torch import nn as nn
from torch import optim
import joblib

is_cuda = torch.cuda.is_available()

if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
#device = torch.device("cuda")
print(device)

def train_lstm (trainloader, valloader, input_dim, rnn_size, output_dim, num_layers, epochs, mse_loss=True, dropout_probability=0, regularization =0, bidirectional = False, flag=True, checkpoint=False):


  model = BasicLSTM(input_dim=input_dim, rnn_size=rnn_size, output_dim=output_dim,
                  num_layers=num_layers, bidirectional=bidirectional, dropout_probability = dropout_probability) 

  if checkpoint:
    best_model = model
    best_accuracy = 0

  model.to(device)
  if mse_loss:
    loss_function = nn.MSELoss()
  else:
    loss_function = nn.CrossEntropyLoss()
  # define an optimizer with L2 regularization
  optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=regularization) 

  for epoch in range(epochs) :
    train_loss = 0
    val_loss = 0
    counter = 0
    
    for batch in trainloader :
      model.train()
      batch_size = len(batch[0])
      max_seq_len = len(batch[0][0])
      input_tensor = torch.zeros(batch_size, max_seq_len, input_dim)
      le = np.array(batch[2])
      lengths = torch.zeros(batch_size, 1, dtype=torch.int64)
      if mse_loss and flag:
        target_tensor = torch.zeros(batch_size, output_dim)
        t_one_hot = [label_to_one_hot(int(label), output_dim) for label in batch[1]]
      else:
        target_tensor = batch[1].float()
      for i in range(batch_size):
        input_tensor[i]=batch[0][i]
        if mse_loss and flag:
          target_tensor[i] = torch.from_numpy(np.array(t_one_hot[i]))
        lengths[i][0] = le[i]
            

      input_tensor = input_tensor.to(device)
      #target_tensor = target_tensor.to(device)
      lengths = lengths.to(device)
      optimizer.zero_grad()
      out = model(input_tensor, lengths)
      out = out.cpu()
      #print("printing out train")
      #print(out, )
      #for i in range(len(out)):
      #   print("sample = ", i, "out =", out[i], "target = ", target_tensor[i])
      #print(target_tensor)
      #print(out)
      loss = loss_function(out, target_tensor)
      loss.backward()
      optimizer.step()
      train_loss = train_loss + loss.item()
      counter = counter + 1
      #print(counter)

    print("epoch : ", epoch, ",training loss = ", train_loss/counter)

    val_counter = 0
    for val_batch in valloader :
      model.eval()
      val_batch_size = len(val_batch[0])
      max_seq_len = len(val_batch[0][0])
      input_tensor = torch.zeros(val_batch_size, max_seq_len, input_dim)
      le = np.array(val_batch[2])
      lengths = torch.zeros(val_batch_size, 1, dtype=torch.int64)
      if mse_loss and flag:
        target_tensor = torch.zeros(val_batch_size, output_dim)
        t_one_hot = [label_to_one_hot(int(label), output_dim) for label in val_batch[1]]
      else:
        target_tensor = val_batch[1].float()
      for i in range(val_batch_size):
        input_tensor[i]=val_batch[0][i]
        if mse_loss and flag:
          target_tensor[i] = torch.from_numpy(np.array(t_one_hot[i]))
        lengths[i][0] = le[i]
        
      input_tensor = input_tensor.to(device)
      #target_tensor = target_tensor.to(device)
      lengths = lengths.to(device)
      out_val = model(input_tensor, lengths)
      out_val = out_val.cpu()
      #print("printing out_val")
      #print(out_val.cpu())
      loss_val = loss_function(out_val, target_tensor)
      val_loss = val_loss + loss_val.item()
      #accuracy = eval_lstm(model, val_batch[0], val_batch[1], val_batch[2], input_dim, output_dim)
      val_counter += 1
      
    if (flag):
      arr, accuracy = eval_lstm(model, valloader, input_dim, output_dim)
      print("Accuracy in validation set = ", accuracy*100,"%")    
    #print("confusion matrix = ", arr)
    print("epoch : ", epoch, ",validation loss = ", val_loss/(val_counter))

    if checkpoint:
      _, accuracy = eval_lstm(model, valloader, input_dim, output_dim)
      if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = copy.deepcopy(model)

  if checkpoint:
    return best_model
  return model

from sklearn.metrics import zero_one_loss

# we implement a function for evaluating our lstm model

def eval_lstm(model, loader, input_dim, output_dim):
  
  model.eval()
  y_arr = []
  y_pred = []
  for batch in loader :
    batch_size = len(batch[0])
    max_seq_len = len(batch[0][0])
    input_tensor = torch.zeros(batch_size, max_seq_len, input_dim)
    le = np.array(batch[2])
    lengths = torch.zeros(batch_size, 1, dtype=torch.int64)
    for i in range(batch_size):
      input_tensor[i]= batch[0][i]
      lengths[i][0] = le[i]
  
    temp = np.array(batch[1])
    for i in range(len(temp)):
      y_arr.append(temp[i])
    #y_arr.append()
    #n_categories = len(set(y_arr))
    #print(input_tensor)
    input_tensor = input_tensor.to(device)
    lengths = lengths.to(device)
  
    out = model(input_tensor, lengths)
    out = out.cpu()
    #y_pred = []
    #print(out)
    for i in range(len(out)):
      #print(out[i])
      probabilities = out[i].detach().view(-1).numpy()
      #print(probabilities, np.argmax(probabilities), y_arr[i])
      y_pred.append(np.argmax(probabilities))

  #print("predictions = ", y_pred)
  #print("true values = ", y)
  count_diff = np.zeros((output_dim, output_dim)) #it is for saving the sum of results
  for i in range(len(y_arr)):
    count_diff[y_arr[i], y_pred[i]] += 1 #we increase the counter in the model of the true digit in the place of the predicted digit
  #print(y_arr,y_pred)
  #return count_diff, 1-zero_one_loss(y_arr, y_pred)
  return count_diff,1-zero_one_loss(y_arr, y_pred)

"""Εκπαιδεύουμε πάνω στο σύνολο εκπαίδευσης των φασματογραφημάτων με σκοπό την πρόβλεψη διαφορετικών κλάσεων μουσικών ειδών"""

mel_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

train_loader1, val_loader1 = torch_train_val_split(mel_specs, 32 ,32, val_size=.33)

ttest_loader1 = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
     train=False,
     class_mapping=class_mapping,
     max_length=-1,
     read_spec_fn=read_mel_spectrogram)

model5_a = train_lstm (train_loader1, val_loader1, input_dim = 128, rnn_size=128, output_dim=10, num_layers=2, epochs=15, dropout_probability=0, regularization = 0.001, bidirectional = False)

"""Εκπαιδεύουμε πάνω στο σύνολο εκπαίδευσης των φασματογραφημάτων beat-synced με σκοπό την πρόβλεψη διαφορετικών κλάσεων μουσικών ειδών"""

beat_mel_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

train_loader2, val_loader2 = torch_train_val_split(beat_mel_specs, 32 ,32, val_size=.33)

ttest_loader2 = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
     train=False,
     class_mapping=class_mapping,
     max_length=-1,
     read_spec_fn=read_mel_spectrogram)

model5_b = train_lstm (train_loader2, val_loader2, input_dim = 128, rnn_size=128, output_dim=10, num_layers=2, mse_loss=True, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False)

"""Εκπαιδεύουμε πάνω στο σύνολο εκπαίδευσης των χρωμογραφημάτων με σκοπό την πρόβλεψη διαφορετικών κλάσεων μουσικών ειδών"""

chroma_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

train_loader3, val_loader3 = torch_train_val_split(chroma_specs, 32 ,32, val_size=.33)

ttest_loader3 = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
     train=False,
     class_mapping=class_mapping,
     max_length=-1,
     read_spec_fn=read_chromagram)

model5_c = train_lstm (train_loader3, val_loader3, input_dim = 12, rnn_size=128, output_dim=10, num_layers=2, mse_loss=False, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False)

"""Εκπαιδεύουμε πάνω στο σύνολο εκπαίδευσης των ενωμένων φασματογραφημάτων και χρωμογραφημάτων με σκοπό την πρόβλεψη διαφορετικών κλάσεων μουσικών ειδών"""

fused_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_fused_spectrogram)

train_loader4, val_loader4 = torch_train_val_split(fused_specs, 32 ,32, val_size=.33)

ttest_loader4 = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
     train=False,
     class_mapping=class_mapping,
     max_length=-1,
     read_spec_fn=read_fused_spectrogram)

model5_d = train_lstm (train_loader4, val_loader4, input_dim =140, rnn_size=128, output_dim=10, num_layers=2, mse_loss=True, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False)

"""# Βήμα 6

Περνάμε τώρα στην αξιολόγηση των τεσσάρων προηγούμενων μοντέλων στα εξής δύο test sets:
- fma_genre_spectrograms_beat/test_labels.txt
- fma_genre_spectrograms/test_labels.txt

Για τον σκοπό αυτό τροποποιούμε την συνάρτηση αξιολόγησης του μοντέλου
"""

from sklearn.metrics import zero_one_loss
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report


# we implement a function for evaluating our lstm model on various metrics

def deep_eval_lstm(model, loader, input_dim, output_dim):
  # create tensors
  input_list = []
  y_arr = []
  lengths_list = []
  for test_sequence, label, length in loader :
    input_list.append(test_sequence)
    y_arr.append(label)
    lengths_list.append(length)

  test_set_size = loader.__len__()
  input_tensor = torch.zeros(test_set_size, len(input_list[0]), input_dim)
  lengths = torch.zeros(test_set_size, 1, dtype=torch.int64)
  for i in range(test_set_size):
    input_tensor[i] = torch.from_numpy(input_list[i])
    lengths[i][0] = lengths_list[i]

  model.eval()
  y_pred = []
  
  input_tensor = input_tensor.to(device)
  lengths = lengths.to(device)

  out = model(input_tensor, lengths)
  out = out.cpu()
  for i in range(len(out)):
    probabilities = out[i].detach().view(-1).numpy()
    y_pred.append(np.argmax(probabilities))

  report = classification_report(y_arr, y_pred, list(range(output_dim)))
  print(report)

  return report, y_arr, y_pred

report1, _, _ = deep_eval_lstm(model5_a, ttest_loader1, 128, 10)

report2, _, _  = deep_eval_lstm(model5_b, ttest_loader2, 128, 10)

report3, _, _  = deep_eval_lstm(model5_c, ttest_loader3, 12, 10)

report4, _, _  = deep_eval_lstm(model5_d, ttest_loader4, 140, 10)

"""# Βήμα 7"""

mel_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

train_loader1, val_loader1 = torch_train_val_split(mel_specs, 32 ,32, val_size=.33)

for data in train_loader1:
    t = (data)
    break

batch_size = len(t[0])
sequence_size = len(t[0][0])
feature_size = len(t[0][0][0])
output_size = 10
print("batch size: ", batch_size)
print("sequence size: ", sequence_size)
print("feature size: ", feature_size)
print("output size: ", output_size)

ttest_loader1 = SpectrogramDataset(
     './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
     train=False,
     class_mapping=class_mapping,
     max_length=sequence_size,
     read_spec_fn=read_mel_spectrogram)

is_cuda = torch.cuda.is_available()
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print("Device: ", device)

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import torch.nn as nn
import math

class BasicCNN(nn.Module):
    def __init__(self, image_x, image_y, output_dim):
        super(BasicCNN, self).__init__()
        kernel_size = [3, 3, 3, 3]
        pool_size = [2, 2, 2, 2]
        self.image_x = image_x
        self.image_y = image_y

        self.model1 = nn.Sequential(
          nn.Conv2d(in_channels=1, out_channels=16, kernel_size=kernel_size[0]),
          nn.BatchNorm2d(num_features=16),
          nn.ReLU(),
          nn.MaxPool2d(kernel_size=pool_size[0])
        )
        out1_image_x = math.floor((((image_x  - (kernel_size[0] - 1)) - (pool_size[0] - 1) - 1) / 2) + 1)
        out1_image_y = math.floor((((image_y  - (kernel_size[0] - 1)) - (pool_size[0] - 1) - 1) / 2) + 1)
        
        self.model2 = nn.Sequential(
          nn.Conv2d(16,32,kernel_size[1]),
          nn.BatchNorm2d(32),
          nn.ReLU(),
          nn.MaxPool2d(pool_size[1])
        )
        out2_image_x = math.floor((((out1_image_x  - (kernel_size[1] - 1)) - (pool_size[1] - 1) - 1) / 2) + 1)
        out2_image_y = math.floor((((out1_image_y  - (kernel_size[1] - 1)) - (pool_size[1] - 1) - 1) / 2) + 1)
        
        self.model3 = nn.Sequential(
          nn.Conv2d(32,64,kernel_size[2]),
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.MaxPool2d(pool_size[2])
        )
        out3_image_x = math.floor((((out2_image_x  - (kernel_size[2] - 1)) - (pool_size[2] - 1) - 1) / 2) + 1)
        out3_image_y = math.floor((((out2_image_y  - (kernel_size[2] - 1)) - (pool_size[2] - 1) - 1) / 2) + 1)

        self.model4 = nn.Sequential(
          nn.Conv2d(64,128,kernel_size[3]),
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.MaxPool2d(pool_size[3])
        )
        out4_image_x = math.floor((((out3_image_x  - (kernel_size[3] - 1)) - (pool_size[3] - 1) - 1) / 2) + 1)
        out4_image_y = math.floor((((out3_image_y  - (kernel_size[3] - 1)) - (pool_size[3] - 1) - 1) / 2) + 1)

        self.final_x = int(out4_image_x)
        self.final_y = int(out4_image_y)
        self.final_features = 128 * self.final_x * self.final_y

        self.linear = nn.Linear(in_features=self.final_features, out_features=output_dim)

    def forward(self, x):
        """ 
            x : 3D numpy array of dimension N x L x D
                N: batch index
                L: sequence index
                D: feature index
            lengths: N x 1
         """
        output1 = self.model1(x)
        # print(output1.shape)
        output2 = self.model2(output1)
        # print(output2.shape)
        output3 = self.model3(output2)
        # print(output3.shape)
        final_layer = self.model4(output3)

        flattened = torch.flatten(final_layer, start_dim=1)
        # print(flattened.shape)
        linear_out = self.linear(flattened)
        # print(linear_out.shape)
        
        return linear_out

from sklearn.metrics import zero_one_loss

# we implement a function for evaluating our CNN model on validation
def validation(model, loader, input_dim, output_dim):
  y_arr = []
  y_pred = []
  
  for batch, labels, _ in loader:
    y_arr = y_arr + [element.item() for element in labels]
    # create tensor
    input_tensor = batch.float().unsqueeze_(1).to(device)
      
    # make predictions on device
    model.eval()
    out = model(input_tensor)
    out = out.cpu()
    for i in range(len(out)):
      probabilities = out[i].detach().view(-1).numpy()
      y_pred.append(np.argmax(probabilities))

  # create confusion matrix and accuracy
  count_diff = np.zeros((output_dim, output_dim))
  for i in range(len(y_arr)):
    count_diff[y_arr[i], y_pred[i]] += 1
  # return the results
  return count_diff,1-zero_one_loss(y_arr, y_pred)

from torch import optim

# define our cnn model
ourCNN = BasicCNN(sequence_size, feature_size, output_size)
ourCNN.to(device)
# set a loss function
loss_function = nn.CrossEntropyLoss()
# define an optimizer with L2 regularization
optimizer = optim.Adam(ourCNN.parameters(), lr=0.0001, weight_decay=0.001) 

epochs = 25
for epoch in range(epochs):
  train_loss = 0
  counter = 0
  ourCNN.train()
  for input, labels,_ in train_loader1:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels
    optimizer.zero_grad()
    out = ourCNN(input_tensor)
    out = out.cpu()
    loss = loss_function(out, target_tensor)
    loss.backward()
    optimizer.step()
    train_loss = train_loss + loss.item()
    counter = counter + 1

  print("epoch : ", epoch, ",training loss = ", train_loss/counter)

  _, val_accuracy = validation(ourCNN, val_loader1, feature_size, output_size)
  print("Accuracy in validation set = ", val_accuracy*100,"%")

from sklearn.metrics import zero_one_loss

# we implement a function for evaluating our CNN model on various metrics
def eval_cnn(model, loader, input_dim, output_dim):
  y_arr = []
  y_pred = []
  input_list = []
  
  for input_id in range(loader.__len__()):
    batch_size = 32

    # create tensors
    test_sequence, label, _ = loader.__getitem__(input_id)
    input_list.append(test_sequence)
    y_arr.append(label)

    if len(input_list)  == batch_size or input_id == (loader.__len__() - 1):
      input_tensor = torch.zeros(len(input_list), 1, len(input_list[0]), input_dim)
      for i in range(len(input_list)):
        input_tensor[i][0] = torch.from_numpy(input_list[i])
      
      # make predictions on device
      model.eval()
      input_tensor = input_tensor.to(device)
      out = model(input_tensor)
      out = out.cpu()
      for i in range(len(out)):
        probabilities = out[i].detach().view(-1).numpy()
        y_pred.append(np.argmax(probabilities))
      input_list = []

  # create confusion matrix and accuracy
  count_diff = np.zeros((output_dim, output_dim))
  for i in range(len(y_arr)):
    count_diff[y_arr[i], y_pred[i]] += 1
  # return the results
  return count_diff,1-zero_one_loss(y_arr, y_pred)

eval_cnn(ourCNN, ttest_loader1, 128, 10)

"""# Βήμα 8"""

data_lab = open('./kaggle/input/patreco3-multitask-affective-music/data/multitask_dataset/train_labels.txt', 'r')
print(data_lab.read())

class MultitaskDataset(Dataset):
    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_fused_spectrogram, target=1):
        t = 'train' if train else 'test'
        p = os.path.join(path, t)
        self.index = os.path.join(path, "{}_labels.txt".format(t))
        self.files, labels = self.get_files_labels(self.index, class_mapping, target)
        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]
        self.feat_dim = self.feats[0].shape[1]
        self.lengths = [len(i) for i in self.feats]
        self.max_length = max(self.lengths) if max_length <= 0 else max_length
        self.zero_pad_and_stack = PaddingTransform(self.max_length)
        #self.label_transformer = LabelTransformer()
        if isinstance(labels, (list, tuple)):
            self.labels = np.array(labels).astype('float')
        # print(self.labels)

    def get_files_labels(self, txt, class_mapping, target):
        with open(txt, 'r') as fd:
            lines = [l.rstrip().split('\t') for l in fd.readlines()[1:]]
        files, labels = [], []
        for l in lines:
            l = l[0].split(',')
            #print(l)
            if target == -1: # take all
              label = l[1:]
            else:
              label = l[target]
            # print(label)
            if class_mapping:
                label = class_mapping[label]
            if not label:
                continue
            # Kaggle automatically unzips the npy.gz format so this hack is needed
            _id = l[0].split('.')[0]
            npy_file = '{}.fused.full.npy'.format(_id)
            files.append(npy_file)
            labels.append(label)
        #print(labels)
        return files, labels

    def __getitem__(self, item):
        # TODO: Inspect output and comment on how the output is formatted
        l = min(self.lengths[item], self.max_length)
        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l

    def __len__(self):
        return len(self.labels)

valence = MultitaskDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',
         train=True,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram,
         target = 1) #valence = 1, energy = 2, danceability = 3

def torch_train_val_test_split(
        dataset, batch_train, batch_eval, batch_test,
        val_size=.2, test_size = .2, shuffle=True, seed=None):
    # Creating data indices for training and validation splits:
    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    val_split = int(np.floor(val_size * dataset_size))
    test_split = int(np.floor(test_size * dataset_size))
    if shuffle:
        np.random.seed(seed)
        np.random.shuffle(indices)
    train_indices = indices[val_split+test_split:]
    val_indices = indices[:val_split]
    test_indices = indices[val_split:test_split+val_split]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)
    test_sampler = SubsetRandomSampler(test_indices)

    train_loader = DataLoader(dataset,
                              batch_size=batch_train,
                              sampler=train_sampler)
    val_loader = DataLoader(dataset,
                            batch_size=batch_eval,
                            sampler=val_sampler)
    test_loader = DataLoader(dataset,
                            batch_size=batch_test,
                            sampler=test_sampler)
    return train_loader, val_loader, test_loader

train_loader_valence, val_loader_valence, test_loader_valence = torch_train_val_test_split(valence, 32 ,32, 32, val_size=.2, test_size=.2)

print(test_loader_valence.__len__())
print(train_loader_valence.__len__())
print(val_loader_valence.__len__())

model_valence = train_lstm (train_loader_valence, val_loader_valence, input_dim = 128, rnn_size=128, output_dim=1, num_layers=2, mse_loss=True, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False, flag=False)

energy = MultitaskDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',
         train=True,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram,
         target = 2) #valence = 1, energy = 2, danceability = 3

train_loader_energy, val_loader_energy, test_loader_energy = torch_train_val_test_split(energy, 32 ,32, 32, val_size=.2, test_size=.2)

model_energy = train_lstm (train_loader_energy, val_loader_energy, input_dim = 128, rnn_size=128, output_dim=1, num_layers=2, mse_loss=True, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False, flag=False)

danceability = MultitaskDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',
         train=True,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram,
         target = 3) #valence = 1, energy = 2, danceability = 3

train_loader_danceability, val_loader_danceability, test_loader_danceability = torch_train_val_test_split(danceability, 32 ,32, 32, val_size=.2, test_size=.2)

model_danceability = train_lstm (train_loader_danceability, val_loader_danceability, input_dim = 128, rnn_size=128, output_dim=1, num_layers=2, mse_loss=True, epochs=15, dropout_probability=0, regularization = 0, bidirectional = False, flag=False)

from sklearn.metrics import zero_one_loss

# we implement a function for evaluating our lstm model

def eval_pred(model, loader, input_dim, output_dim):
  
 
  y_arr = []
  y_pred = []
  for batch in loader :
    batch_size = len(batch[0])
    max_seq_len = len(batch[0][0])
    input_tensor = torch.zeros(batch_size, max_seq_len, input_dim)
    le = np.array(batch[2])
    lengths = torch.zeros(batch_size, 1, dtype=torch.int64)
    for i in range(batch_size):
      input_tensor[i]= batch[0][i]
      lengths[i][0] = le[i]
  
    temp = np.array(batch[1])
    for i in range(len(temp)):
      y_arr.append(temp[i])
    #y_arr.append()
    #n_categories = len(set(y_arr))
    #print(input_tensor)
    input_tensor = input_tensor.to(device)
    lengths = lengths.to(device)
   
    model.eval()
    out = model(input_tensor, lengths)
    out = out.cpu()
    #print(out)

    for i in range(len(out)):
      pred = out[i].detach().view(-1).numpy()
      # print(pred)
      y_pred.append(pred)

  #print(len(y_arr))
  #print(len(y_pred))
  return y_arr, y_pred

from scipy import stats

labels_valence, predictions_valence = eval_pred(model_valence, test_loader_valence, 128, 1)
#print(predictions_valence)
spearman_valence, _ = stats.spearmanr(labels_valence, predictions_valence)
labels_energy, predictions_energy = eval_pred(model_energy, test_loader_energy, 128, 1)
spearman_energy, _ = stats.spearmanr(labels_energy, predictions_energy)
labels_danceability, predictions_danceability = eval_pred(model_danceability, test_loader_danceability, 128, 1)
spearman_danceability, _ = stats.spearmanr(labels_danceability, predictions_danceability)

print("The Spearman correlation between the ground true values and the predicted values for valence is ", spearman_valence)
print("The Spearman correlation between the ground true values and the predicted values for energy is ", spearman_energy)
print("The Spearman correlation between the ground true values and the predicted values for danceability is ", spearman_danceability)

print("The Mean Spearman correlation between the ground true values and the predicted values is ", ((abs(spearman_valence) + abs(spearman_energy) + abs(spearman_danceability))/3)*100, "%")
#print(predictions_valence[1])

"""Προσαργμογή του CNN μοντέλου"""

batch_size = None
sequence_size = None
feature_size = None

for input, labels, length in train_loader_valence: #, val_loader_valence, test_loader_valence
  batch_size = len(input)
  sequence_size = len(input[0])
  feature_size = len(input[0][0])
  break
  
output_size = 1
print("batch size: ", batch_size)
print("sequence size: ", sequence_size)
print("feature size: ", feature_size)
print("output size: ", output_size)

from torch import optim

def train_cnn(model, train_loader, val_loader):
  model.to(device)
  # set a loss function for regression
  loss_function = nn.MSELoss()
  # define an optimizer with L2 regularization
  optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001) 

  epochs = 50
  for epoch in range(epochs):
    train_loss = 0
    counter = 0
    model.train()
    for input, labels,_ in train_loader:
      # print(input.float().unsqueeze_(1).shape)
      input_tensor = input.float().unsqueeze_(1).to(device)
      target_tensor = labels.float()
      optimizer.zero_grad()
      out = model(input_tensor)
      out = out.cpu().flatten()
      loss = loss_function(out, target_tensor)
      loss.backward()
      optimizer.step()
      train_loss = train_loss + loss.item()
      counter = counter + 1

    if epoch % 5 == 0:
      print("epoch : ", epoch, ",training loss = ", train_loss/counter)


    val_loss = 0
    val_counter = 0
    model.eval()
    for input, labels,_ in val_loader:
      # print(input.float().unsqueeze_(1).shape)
      input_tensor = input.float().unsqueeze_(1).to(device)
      target_tensor = labels.float()
      out = model(input_tensor)
      out = out.cpu().flatten()
      #print(out)
      loss = loss_function(out, target_tensor)
      val_loss = val_loss + loss.item()
      val_counter = val_counter + 1

    if epoch % 5 == 0:
      print("epoch : ", epoch, ",validation loss = ", val_loss/val_counter)
  return model

non_trained = BasicCNN(sequence_size, feature_size, output_size)
valence_CNN = train_cnn(non_trained, train_loader_valence, val_loader_valence)

non_trained = BasicCNN(sequence_size, feature_size, output_size)
energy_CNN = train_cnn(non_trained, train_loader_energy, val_loader_energy)

non_trained = BasicCNN(sequence_size, feature_size, output_size)
danceability_CNN = train_cnn(non_trained, train_loader_danceability, val_loader_danceability)

from sklearn.metrics import zero_one_loss

# we implement a function for getting our CNN model predictions
def cnn_pred(model, loader):
  y_arr = []
  y_pred = []
  
  for batch, labels, _ in loader:
    y_arr = y_arr + [element.item() for element in labels]
    # create tensor
    input_tensor = batch.float().unsqueeze_(1).to(device)
      
    # make predictions on device
    model.eval()
    out = model(input_tensor)
    out = out.cpu().flatten()
    for i in range(len(out)):
      pred = out[i].detach().view(-1).numpy()
      # print(pred)
      y_pred.append(pred)

  
  # return the results
  return y_arr, y_pred

from scipy import stats

labels_valence, predictions_valence = cnn_pred(valence_CNN, test_loader_valence)
print(predictions_valence)
spearman_valence, _ = stats.spearmanr(labels_valence, predictions_valence)
labels_energy, predictions_energy = cnn_pred(energy_CNN, test_loader_energy)
spearman_energy, _ = stats.spearmanr(labels_energy, predictions_energy)
labels_danceability, predictions_danceability = cnn_pred(danceability_CNN, test_loader_danceability)
spearman_danceability, _ = stats.spearmanr(labels_danceability, predictions_danceability)

print("The Spearman correlation between the ground true values and the predicted values for valence is ", spearman_valence)
print("The Spearman correlation between the ground true values and the predicted values for energy is ", spearman_energy)
print("The Spearman correlation between the ground true values and the predicted values for danceability is ", spearman_danceability)

print("The Mean Spearman correlation between the ground true values and the predicted values is ", ((abs(spearman_valence) + abs(spearman_energy) + abs(spearman_danceability))/3)*100, "%")
# print(predictions_valence)

"""# Βήμα 9α

Επιλέγουμε το cnn μοντέλο καθώς είχε την υψηλότερη ακρίβεια
"""

mel_specs = SpectrogramDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

train_loader1, val_loader1 = torch_train_val_split(mel_specs, 32 ,32, val_size=.33)

for data in train_loader1:
    t = (data)
    break

batch_size = len(t[0])
sequence_size = len(t[0][0])
feature_size = len(t[0][0][0])
output_size = 10
print("batch size: ", batch_size)
print("sequence size: ", sequence_size)
print("feature size: ", feature_size)
print("output size: ", output_size)

from torch import optim

# define our cnn model
cnn_on_genre = BasicCNN(sequence_size, feature_size, output_size)
# keep checkpoint
best_cnn_on_genre = BasicCNN(sequence_size, feature_size, output_size)
best_accuracy = 0
cnn_on_genre.to(device)
# set a loss function
loss_function = nn.CrossEntropyLoss()
# define an optimizer with L2 regularization
optimizer = optim.Adam(cnn_on_genre.parameters(), lr=0.0001, weight_decay=0.001) 

epochs = 25
for epoch in range(epochs):
  train_loss = 0
  counter = 0
  cnn_on_genre.train()
  for input, labels,_ in train_loader1:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels
    optimizer.zero_grad()
    out = cnn_on_genre(input_tensor)
    out = out.cpu()
    loss = loss_function(out, target_tensor)
    loss.backward()
    optimizer.step()
    train_loss = train_loss + loss.item()
    counter = counter + 1

  print("epoch : ", epoch, ",training loss = ", train_loss/counter)

  _, val_accuracy = validation(cnn_on_genre, val_loader1, feature_size, output_size)
  print("Accuracy in validation set = ", val_accuracy*100,"%")

  if val_accuracy > best_accuracy:
    best_accuracy = val_accuracy
    best_cnn_on_genre = copy.deepcopy(cnn_on_genre)

eval_cnn(best_cnn_on_genre, ttest_loader1, 128, 10)

batch_size = None
sequence_size = None
feature_size = None

for input, labels, length in train_loader_valence: #, val_loader_valence, test_loader_valence
  batch_size = len(input)
  sequence_size = len(input[0])
  feature_size = len(input[0][0])
  break
  
output_size = 1
print("batch size: ", batch_size)
print("sequence size: ", sequence_size)
print("feature size: ", feature_size)
print("output size: ", output_size)

# get the parameters from the trained on genre dataset and setting them to the new model
transfered_cnn = BasicCNN(sequence_size, feature_size, output_size) 
for model_param, trained_param in zip(transfered_cnn.parameters(), best_cnn_on_genre.parameters()):
  if model_param.size() == trained_param.size():
    print(model_param.size(), trained_param.size())
    model_param.data = trained_param.data

transfered_cnn.to(device)
# set a loss function for regression
loss_function = nn.MSELoss()
# define an optimizer with L2 regularization
optimizer = optim.Adam(transfered_cnn.parameters(), lr=0.0001, weight_decay=0.001) 

epochs = 10
for epoch in range(epochs):
  train_loss = 0
  counter = 0
  transfered_cnn.train()
  for input, labels,_ in train_loader_valence:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels.float()
    optimizer.zero_grad()
    out = transfered_cnn(input_tensor)
    out = out.cpu().flatten()
    loss = loss_function(out, target_tensor)
    loss.backward()
    optimizer.step()
    train_loss = train_loss + loss.item()
    counter = counter + 1

  print("epoch : ", epoch, ",training loss = ", train_loss/counter)


  val_loss = 0
  val_counter = 0
  transfered_cnn.eval()
  for input, labels,_ in val_loader_valence:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels.float()
    out = transfered_cnn(input_tensor)
    out = out.cpu().flatten()
    #print(out)
    loss = loss_function(out, target_tensor)
    val_loss = val_loss + loss.item()
    val_counter = val_counter + 1

  print("epoch : ", epoch, ",validation loss = ", val_loss/val_counter)

from scipy import stats

labels_valence, predictions_valence = cnn_pred(transfered_cnn, test_loader_valence)
spearman_valence, _ = stats.spearmanr(labels_valence, predictions_valence)
# labels_energy, predictions_energy = cnn_pred(transfered_cnn, test_loader_energy)
# spearman_energy, _ = stats.spearmanr(labels_energy, predictions_energy)
# labels_danceability, predictions_danceability = cnn_pred(transfered_cnn, test_loader_danceability)
# spearman_danceability, _ = stats.spearmanr(labels_danceability, predictions_danceability)


print("The Spearman correlation between the ground true values and the predicted values for valence is ", spearman_valence)
# print("The Spearman correlation between the ground true values and the predicted values for energy is ", spearman_energy)
# print("The Spearman correlation between the ground true values and the predicted values for danceability is ", spearman_danceability)

"""# Βήμα 9β"""

multitask_dataset = MultitaskDataset(
         './kaggle/input/patreco3-multitask-affective-music/data/multitask_dataset/',
         train=True,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram,
         target = -1) #valence = 1, energy = 2, danceability = 3 all_toghether = -1

train_loader, val_loader, test_loader = torch_train_val_test_split(multitask_dataset, 32 ,32, 32, val_size=.2, test_size=.2)

print("Train dataset length: {}".format(train_loader.__len__()))
print("Validation dataset length: {}".format(val_loader.__len__()))
print("Test dataset length: {}".format(test_loader.__len__()))
print("-------------------")
batch_size = None
sequence_size = None
feature_size = None
output_size = None

for input, labels, length in train_loader: #, val_loader, test_loader
  batch_size = len(input)
  sequence_size = len(input[0])
  feature_size = len(input[0][0])
  output_size = len(labels[0])
  break
  
print("batch size: ", batch_size)
print("sequence size: ", sequence_size)
print("feature size: ", feature_size)
print("output size: ", output_size)

from torch import optim

multi_model = BasicCNN(image_x=sequence_size, image_y=feature_size, output_dim=output_size)
multi_model.to(device)

# set a loss function for each task
weights = [0.33, 0.33, 0.33]
valence_loss_function = nn.MSELoss()
energy_loss_function = nn.MSELoss()
danceability_loss_function = nn.MSELoss()

# define an optimizer with L2 regularization
optimizer = optim.Adam(multi_model.parameters(), lr=0.0001, weight_decay=0.001) 

epochs = 50
for epoch in range(epochs):
  train_loss = 0
  avg_valence_loss = 0
  avg_energy_loss = 0
  avg_danceability_loss = 0
  counter = 0
  multi_model.train()
  for input, labels,_ in train_loader:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels.float()
    optimizer.zero_grad()
    out = multi_model(input_tensor)
    out = out.cpu()
    valence_loss = valence_loss_function(out[:, 0], target_tensor[:, 0])
    energy_loss = energy_loss_function(out[:, 1], target_tensor[:, 1])
    danceability_loss = danceability_loss_function(out[:, 2], target_tensor[:, 2])
    loss = weights[0]*valence_loss + weights[1]*energy_loss + weights[2]*danceability_loss
    loss.backward()
    optimizer.step()
    train_loss = train_loss + loss.item()
    avg_valence_loss = avg_valence_loss + valence_loss.item()
    avg_energy_loss = avg_energy_loss + energy_loss.item()
    avg_danceability_loss = avg_danceability_loss + danceability_loss.item()
    counter = counter + 1

  if epoch % 5 == 0:
    print("Epoch: ", epoch)
    print("Training")
    print("valence_loss: {}, energy_loss: {}, danceability_loss: {}".format(avg_valence_loss/counter, avg_energy_loss/counter, avg_danceability_loss/counter))
    print("Total training loss = ", train_loss/counter)

  val_loss = 0
  val_avg_valence_loss = 0
  val_avg_energy_loss = 0
  val_avg_danceability_loss = 0
  val_counter = 0
  multi_model.eval()
  for input, labels,_ in val_loader:
    # print(input.float().unsqueeze_(1).shape)
    input_tensor = input.float().unsqueeze_(1).to(device)
    target_tensor = labels.float()
    out = multi_model(input_tensor)
    out = out.cpu()
    #print(out)
    valence_loss = valence_loss_function(out[:, 0], target_tensor[:, 0])
    energy_loss = energy_loss_function(out[:, 1], target_tensor[:, 1])
    danceability_loss = danceability_loss_function(out[:, 2], target_tensor[:, 2])
    loss = weights[0]*valence_loss + weights[1]*energy_loss + weights[2]*danceability_loss
    val_loss = val_loss + loss.item()
    val_avg_valence_loss = val_avg_valence_loss + valence_loss.item()
    val_avg_energy_loss = val_avg_energy_loss + energy_loss.item()
    val_avg_danceability_loss = val_avg_danceability_loss + danceability_loss.item()
    val_counter = val_counter + 1

  if epoch % 5 == 0:
    print("Validation")
    print("valence_loss: {}, energy_loss: {}, danceability_loss: {}".format(val_avg_valence_loss/val_counter, val_avg_energy_loss/val_counter, val_avg_danceability_loss/val_counter))
    print("Total validation loss = ", val_loss/val_counter)

# Get our multi task trained model to predict on test dataset

y_arr = []
y_pred = []
 
for batch, labels, _ in test_loader:
  y_arr = y_arr + [label.numpy() for label in labels]
  # create tensor
  input_tensor = batch.float().unsqueeze_(1).to(device)
    
  # make predictions on device
  multi_model.eval()
  out = multi_model(input_tensor)
  out = out.cpu()
  for i in range(len(out)):
    pred = out[i].detach().view(-1).numpy()
    y_pred.append(pred)

print("Ground truth", y_arr)
print("Predictions", y_pred)

from scipy import stats

valence_ground = [y[0] for y in y_arr]
energy_ground = [y[1] for y in y_arr]
danceability_ground = [y[2] for y in y_arr]

valence_pred = [pred[0] for pred in y_pred]
energy_pred = [pred[1] for pred in y_pred]
danceability_pred = [pred[2] for pred in y_pred]


valence_spearman, _ = stats.spearmanr(valence_ground, valence_pred)
energy_spearman, _ = stats.spearmanr(energy_ground, energy_pred)
danceability_spearman, _ = stats.spearmanr(danceability_ground, danceability_pred)

print("The Spearman correlation between the ground true values and the predicted values for valence is\n", valence_spearman)
print("The Spearman correlation between the ground true values and the predicted values for energy is\n", energy_spearman)
print("The Spearman correlation between the ground true values and the predicted values for danceability is\n", danceability_spearman)